{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e76a4e7",
   "metadata": {},
   "source": [
    "<div \n",
    "     style=\"padding: 20px; \n",
    "            color: yellow;\n",
    "            margin: 0;\n",
    "            font-size: 250%;\n",
    "            text-align: center;\n",
    "            display: fill;\n",
    "            border-radius: 5px;\n",
    "            background-color: #0daae3;\n",
    "            overflow: hidden;\n",
    "            font-weight: 700;\n",
    "            border: 5px solid black;\"\n",
    "     >\n",
    "            Candidate ReRank Model using Handcrafted Rules\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a4535",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc00e37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T14:51:29.427219Z",
     "start_time": "2022-11-11T14:51:28.205604Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle \n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import cudf\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bda1e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T14:51:31.218334Z",
     "start_time": "2022-11-11T14:51:31.202637Z"
    }
   },
   "outputs": [],
   "source": [
    "path_project = r'/Users/liliangshan/workspace/python/01_datasets/otto-recommender-system'\n",
    "\n",
    "# path dir\n",
    "path_row_data = os.path.join(path_project, 'row_data')\n",
    "path_new_data = os.path.join(path_project, 'new_data')\n",
    "path_results  = os.path.join(path_project, 'results')\n",
    "\n",
    "# path row_data\n",
    "path_train = os.path.join(path_row_data, 'train.jsonl')\n",
    "path_test  = os.path.join(path_row_data, 'test.jsonl')\n",
    "path_sample_submission = os.path.join(path_row_data, 'sample_submission.csv')\n",
    "\n",
    "# parquet 格式的文件存放路径\n",
    "path_parquet = os.path.join(path_new_data, 'parquet')\n",
    "path_parquet_train = os.path.join(path_parquet, 'train')\n",
    "path_parquet_test = os.path.join(path_parquet, 'test')\n",
    "\n",
    "path_top_40_carts_orders = os.path.join(path_new_data, 'top_40_carts_orders')\n",
    "path_top_40_buy2buy = os.path.join(path_new_data, 'top_40_buy2buy')\n",
    "path_top_40_clicks = os.path.join(path_new_data, 'top_40_clicks')\n",
    "\n",
    "VERSION = 1\n",
    "\n",
    "path_submission = os.path.join(path_results, 'version_{}'.format(VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef1d50d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T14:51:32.525043Z",
     "start_time": "2022-11-11T14:51:32.515439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129个数据集将分为22个部分进行处理\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob.glob('{}/*'.format(path_parquet_train)))\n",
    "chunk = int(np.ceil(len(files)/6))\n",
    "\n",
    "print(\"{}个数据集将分为{}个部分进行处理\".format(len(files), chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d0745",
   "metadata": {},
   "source": [
    "## Step 1 Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837eee80",
   "metadata": {},
   "source": [
    "### “点击/收藏/购买”共现矩阵-类别加权\n",
    "假设用户当前浏览的商品会影响用户后续浏览商品的选择，因此构造商品-商品共现矩阵，其中用户对不同商品的动作代表的商品间的不同权重，因此可以通过这种逻辑来基于用户当前浏览的商品来预测下一个客户可能会浏览的商品\n",
    "+ 这里用于当前商品的下一个浏览商品的推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "type_labels = {\"clicks\":0, \"carts\":1, \"orders\":2} # 类别映射字典\n",
    "type_weight = {0:1, 1:6, 2:3} # 权重映射字典\n",
    "\n",
    "disk_pices = 4\n",
    "size = 1.86e6/disk_pices\n",
    "\n",
    "# 分块计算共现矩阵\n",
    "for part in range(disk_pices):\n",
    "    \n",
    "    # 分块读取数据集\n",
    "    for j in range(6):\n",
    "        a = j*chunk\n",
    "        b = min((j+1)*chunk, len(files))\n",
    "        print(\"正在处理第{}-{}部分的数据\".format(a, b))\n",
    "\n",
    "        for k in range(a, b):\n",
    "            # 读取数据\n",
    "            df = cudf.read_parquet(files[k])\n",
    "            df.ts = (df.ts/1000).astype('int32')\n",
    "            df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # 对数量短的session进行过滤\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n', axis=1)\n",
    "            # 创建商品对\n",
    "            df = pd.merge(left=df, right=df, on='session')\n",
    "            df = df.loc[((df.ts_x-df.ts_y).abs()<24*60*60)&(df.aid_x!=df.aid_y)]\n",
    "            # 分步统计，节省内存\n",
    "            df = df.loc[(df.aid_x >= part*size)&(df.aid_x < (part+1)*size)]\n",
    "            # 分配权重\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # 合并分块\n",
    "            if k == a:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                #tmp2 = pd.concat([tmp2, df]).filna(0)\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ' ', end='')\n",
    "        print()\n",
    "        if a==0: tmp = tmp2\n",
    "        else:  tmp = tmp.add(tmp2, fill_vale=0)  # tmp = pd.concat([tmp, tmp]).filna(0)            # tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # 将矩阵转换为dict\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # 保存前40\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.groupby('aid_x').aid_y.cumcont()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n', axis=1)\n",
    "    # 分块保存在磁盘中\n",
    "    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n",
    "    os.makedirs(path_top_40_carts_orders, exist_ok=True)\n",
    "    with open(os.path.join(path_top_40_carts_orders, 'top_40_carts_orders_v{}_{}.pkl'.format(VERSION, part), 'wb')) as f:\n",
    "        pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb98563",
   "metadata": {},
   "source": [
    "### “收藏/购买”共现矩阵\n",
    "假设用户当前收藏或购买过的商品会影响用户后续的收藏/购买，因此构造商品-商品共现矩阵，其中这里假定收藏/购买的权重一致，因此可以通过这种逻辑来基于用户当前收藏/购买的商品来预测用户下一个可能会收藏/购买的商品。\n",
    "+ 这里用户预测用户下一个可能收藏/购买的商品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04427aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "type_labels = {\"clicks\":0, \"carts\":1, \"orders\":2} \n",
    "\n",
    "disk_pices = 4\n",
    "size = 1.86e6/disk_pices\n",
    "\n",
    "# 分块计算共现矩阵\n",
    "for part in range(disk_pices):\n",
    "    \n",
    "    # 分块读取数据集\n",
    "    for j in range(6):\n",
    "        a = j*chunk\n",
    "        b = min((j+1)*chunk, len(files))\n",
    "        print(\"正在处理第{}-{}部分的数据\".format(a, b))\n",
    "\n",
    "        for k in range(a, b):\n",
    "            # 读取数据\n",
    "            df = cudf.read_parquet(files[k])\n",
    "            df.ts = (df.ts/1000).astype('int32')\n",
    "            df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # 对数量短的session进行过滤\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n', axis=1)\n",
    "            # 创建商品对\n",
    "            df = pd.merge(left=df, right=df, on='session')\n",
    "            df = df.loc[((df.ts_x-df.ts_y).abs()<24*60*60)&(df.aid_x!=df.aid_y)]\n",
    "            # 分步统计，节省内存\n",
    "            df = df.loc[(df.aid_x >= part*size)&(df.aid_x < (part+1)*size)]\n",
    "            # 分配权重\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # 合并分块\n",
    "            if k == a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ' ', end='')\n",
    "        print()\n",
    "        if a==0: tmp = tmp2\n",
    "        else:  tmp = tmp.add(tmp2, fill_vale=0)  \n",
    "        gc.collect()\n",
    "    # 将矩阵转换为dict\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # 保存前40\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.groupby('aid_x').aid_y.cumcont()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n', axis=1)\n",
    "    # 分块保存在磁盘中\n",
    "    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n",
    "    os.makedirs(path_top_40_buy2buy, exist_ok=True)\n",
    "    with open(os.path.join(path_top_40_buy2buy, 'top_40_buy2buy_v{}_{}.pkl'.format(VERSION, part), 'wb')) as f:\n",
    "        pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e4bad",
   "metadata": {},
   "source": [
    "### “点击/收藏/购买”共现矩阵-时间加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "type_labels = {\"clicks\":0, \"carts\":1, \"orders\":2} \n",
    "\n",
    "disk_pices = 4\n",
    "size = 1.86e6/disk_pices\n",
    "\n",
    "# 分块计算共现矩阵\n",
    "for part in range(disk_pices):\n",
    "    \n",
    "    # 分块读取数据集\n",
    "    for j in range(6):\n",
    "        a = j*chunk\n",
    "        b = min((j+1)*chunk, len(files))\n",
    "        print(\"正在处理第{}-{}部分的数据\".format(a, b))\n",
    "\n",
    "        for k in range(a, b):\n",
    "            # 读取数据\n",
    "            df = cudf.read_parquet(files[k])\n",
    "            df.ts = (df.ts/1000).astype('int32')\n",
    "            df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # 对数量短的session进行过滤\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n', axis=1)\n",
    "            # 创建商品对\n",
    "            df = pd.merge(left=df, right=df, on='session')\n",
    "            df = df.loc[((df.ts_x-df.ts_y).abs()<24*60*60)&(df.aid_x!=df.aid_y)]\n",
    "            # 分步统计，节省内存\n",
    "            df = df.loc[(df.aid_x >= part*size)&(df.aid_x < (part+1)*size)]\n",
    "            # 分配权重\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # 合并分块\n",
    "            if k == a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ' ', end='')\n",
    "        print()\n",
    "        if a==0: tmp = tmp2\n",
    "        else:  tmp = tmp.add(tmp2, fill_vale=0)  \n",
    "        gc.collect()\n",
    "    # 将矩阵转换为dict\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # 保存前40\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.groupby('aid_x').aid_y.cumcont()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n', axis=1)\n",
    "    # 分块保存在磁盘中\n",
    "    df = tmp.to_pandas().groupby('aid_x').aid_y.apply(list)\n",
    "    os.makedirs(path_top_40_clicks, exist_ok=True)\n",
    "    with open(os.path.join(path_top_40_clicks, 'top_40_clicks_v{}_{}.pkl'.format(VERSION, part), 'wb')) as f:\n",
    "        pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df04cca",
   "metadata": {},
   "source": [
    "## Setp 2 ReRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('{}/*'.format(path_parquet_test))):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True).astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "df_test = load_test()\n",
    "print('Test data has shape',df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共现矩阵的结果\n",
    "\n",
    "top_20_clicks = pickle.load(open(os.path.join(path_top_40_clicks,\n",
    "                                'top_40_clicks_v{}_0.pkl'.foramt(VERSION)), 'rb'))\n",
    "for k in range(1,disk_pices): \n",
    "    top_20_clicks.update(pickle.load(open(os.path.join(path_top_40_clicks,\n",
    "                                'top_40_clicks_v{}_{}.pkl'.foramt(VERSION, k)), 'rb')))\n",
    "\n",
    "top_20_buys = pickle.load(open(os.path.join(path_top_40_carts_orders,\n",
    "                                'top_40_carts_orders_v{}_0.pkl'.foramt(VERSION)), 'rb'))\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_buys.update( pickle.load(open(os.path.join(path_top_40_carts_orders,\n",
    "                                'top_40_carts_orders_v{}_{}.pkl'.foramt(VERSION, k)), 'rb')))\n",
    "\n",
    "top_20_buy2buy = pickle.load(open(os.path.join(path_top_40_buy2buy,\n",
    "                                'top_40_buy2buy_v{}_{}.pkl'.foramt(VERSION, k)), 'rb'))\n",
    "\n",
    "# 测试集中的热门点击\n",
    "top_clicks = df_test.loc[df_test['type']=='clicks','aid'].value_counts().index.values[:20]\n",
    "top_orders = df_test.loc[df_test['type']=='orders','aid'].value_counts().index.values[:20]\n",
    "\n",
    "print('共现矩阵的长度:')\n",
    "len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28259e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-13T04:48:19.398332Z",
     "start_time": "2022-11-13T04:48:19.378989Z"
    }
   },
   "outputs": [],
   "source": [
    "type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # 用户历史浏览商品记录及类型\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # 通过权重进行重排序\n",
    "    if len(unique_aids)>=20:\n",
    "        # 时间加权\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True)-1\n",
    "        aids_temp = Counter()\n",
    "        # 使用商品出现次数及其类型进行重排序\n",
    "        for aid, w, t in zip(aids, weight, types):\n",
    "            aids_temp[aid] += w*type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # 使用“点击/收藏/购买”共现矩阵\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # 重排序\n",
    "    top_aids2 = [aid2 for aid2, cnt in Count(aids2).most_common(20) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # 使用测试集合的top20的点击\n",
    "    return result + list(top_clicks)[:20-len(result)]\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # 用户列浏览商品记录和类型\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type']=='carts')|df['type']=='orders']\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # 通过权重进行重排序\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # 使用buy2buy矩阵进行重排序\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # “点击/收藏/购买”共现矩阵\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # 使用buy2buy矩阵进行重排序\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # 重排序\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:20-len(unique_aids)]\n",
    "    \n",
    "    return result + list(top_orders)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75537d",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce76349",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c557ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(os.path.join(path_submission, time.strftime('submission_%Y%m%d%H%M.csv')), index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:otto]",
   "language": "python",
   "name": "conda-env-otto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
